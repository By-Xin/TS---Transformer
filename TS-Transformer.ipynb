{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS Transformers\n",
    "\n",
    "> *注：本代码在编写过程中出现的Crossing Transformer即为文章中提出的TS-Transformer。在编写代码时未发现Crossing Transformer这一名称已在其他工作中使用，因此在文章撰写时改名为TS-Transformer。代码中未及时替换导致的误解作者表示抱歉。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_LEN = 2000\n",
    "\n",
    "SAMPLE_SIZE = 200\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "LOOK_BACK = 45\n",
    "\n",
    "K = 150\n",
    "\n",
    "EARLYSTOP_PATIENCE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T08:20:34.591425500Z",
     "start_time": "2024-03-22T08:20:21.560224600Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random \n",
    "import time\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "####################################### Random Seed #######################################\n",
    "seed = 3407\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "####################################### Import Data #######################################\n",
    "file_path = 'label.csv'\n",
    "data_raw = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_raw.drop('label', axis=1)\n",
    "df = df.drop('Volume', axis=1)\n",
    "df = df.dropna()\n",
    "df.head()\n",
    "\n",
    "############### Data Preprocessing #################\n",
    "df[['High', 'Open', 'Low', 'Close', 'Avg']] = np.log(df[['High', 'Open', 'Low', 'Close', 'Avg']]) \n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df['year'] = pd.to_datetime(df['date']).dt.year\n",
    "df['month'] = pd.to_datetime(df['date']).dt.month\n",
    "df['day'] = pd.to_datetime(df['date']).dt.day\n",
    "\n",
    "df['year'] = df['year'].astype(int)\n",
    "df['month'] = df['month'].astype(int)\n",
    "df['day'] = df['day'].astype(int)\n",
    "\n",
    "df.drop('date', axis=1, inplace=True)\n",
    "\n",
    "df = df[['year', 'month', 'day', 'stock'] + [col for col in df.columns if col not in ['year', 'month', 'day', 'stock']]]\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data, lookback, predict_len=1):\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        self.stock_idx = []\n",
    "        self.lookback = lookback\n",
    "        self.stock_encoder = LabelEncoder()\n",
    "        self.minmaxscale = MinMaxScaler()\n",
    "        self.dta = data\n",
    "        self.predict_len = predict_len\n",
    "        self.dta['stock_enc'] = self.stock_encoder.fit_transform(self.dta['stock'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dta.groupby(by=[\"stock_enc\"]))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_stock_data = self.dta[self.dta[\"stock_enc\"] == idx]\n",
    "\n",
    "        start = np.random.randint(0, idx_stock_data.shape[0] - self.lookback)\n",
    "        stock_name = str(idx_stock_data[['stock']][start:start+self.predict_len].values.item()) # 获取第idx个股票的股票编号\n",
    "        index_input = torch.tensor([i for i in range(start, start+self.lookback)]) # 获得第idx个股票从start到start+lookback (input)的编号索引\n",
    "        index_target = torch.tensor([i for i in range(start + self.lookback, start + self.lookback + self.predict_len)]) # target 即 input 后predict_len天的数据\n",
    "        _input = torch.tensor(self.dta[self.dta['stock_enc'] == idx][['year','month' ,'day', 'High', 'Open', 'Low', 'Close', 'Avg']][start:start+self.lookback].values) # 获取第idx个股票从start到start+lookback的数据，其格式为torch.tensor\n",
    "        \n",
    "        # target.shape:(batch_size, predict_len, features)\n",
    "        target = torch.tensor(self.dta[self.dta['stock_enc'] == idx][['year','month' ,'day', 'High', 'Open', 'Low', 'Close', 'Avg']][start+self.lookback:start+self.lookback+1].values) # 获取第idx个股票从start+lookback到start+lookback+prelen的数据，其格式为torch.tensor\n",
    "        \n",
    "        return index_input, index_target, _input, target, stock_name   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_unique = df['stock'].unique()\n",
    "selected_stocks = random.sample(stock_unique.tolist(), SAMPLE_SIZE)\n",
    "df_selected = df[df['stock'].isin(selected_stocks)]\n",
    "\n",
    "df_selected = df_selected.groupby('stock').filter(lambda x: len(x) >= FILTER_LEN)\n",
    "\n",
    "# 假设df是经过修改后包含'year'、'month'、'day'列的DataFrame\n",
    "# 首先，我们需要将这些列重新组合为日期，以便确定唯一的日期\n",
    "df_selected.loc[:, 'combined_date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "# 获取唯一日期\n",
    "unique_dates = df_selected['combined_date'].unique()\n",
    "train_dates = unique_dates[:int(len(unique_dates) * 0.8)]\n",
    "# print(train_dates)\n",
    "test_dates = unique_dates[int(len(unique_dates) * 0.8):]\n",
    "\n",
    "# 使用组合后的日期来切分训练集和测试集\n",
    "train_df = df_selected[df_selected['combined_date'].isin(train_dates)].copy()\n",
    "test_df = df_selected[df_selected['combined_date'].isin(test_dates)].copy()\n",
    "\n",
    "# 删除'combined_date'列\n",
    "train_df.drop('combined_date', axis=1, inplace=True)\n",
    "test_df.drop('combined_date', axis=1, inplace=True)\n",
    "\n",
    "train_df = train_df.groupby('stock').filter(lambda x: len(x) >= 150)\n",
    "test_df = test_df.groupby('stock').filter(lambda x: len(x) >= 150)\n",
    "\n",
    "print(f\"train_df.shape:{train_df.shape}\")\n",
    "\n",
    "train_dataset = StockDataset(train_df, lookback=LOOK_BACK)  # 以30天为一个查看窗口\n",
    "test_dataset = StockDataset(test_df, lookback=LOOK_BACK)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T05:36:48.526779100Z",
     "start_time": "2024-03-23T05:36:48.483050200Z"
    }
   },
   "outputs": [],
   "source": [
    "df_selected['NewFeature'] = df_selected['Close'].notna().astype(int)\n",
    "df_pivot = df_selected.pivot(index='stock', columns='combined_date', values='NewFeature')\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(df_pivot, annot=False, fmt=\".2f\", cmap=\"YlGnBu\")\n",
    "plt.title('Stock Data Usage Heatmap')\n",
    "plt.xlabel('Stock Code')\n",
    "plt.ylabel('Date')\n",
    "plt.show()\n",
    "\n",
    "print(f\"train_df.shape:{train_df.shape}\")\n",
    "print(f\"train stock num:{train_df['stock'].nunique()}\")\n",
    "print(f\"test_df.shape:{test_df.shape}\")\n",
    "print(f\"test stock num:{test_df['stock'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T02:31:54.373657800Z",
     "start_time": "2024-03-22T02:31:54.327800600Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 通用SelfAttention模块\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "        \n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "    \n",
    "    def forward(self, values, keys, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        # Split the embedding into `self.heads` pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # Query-Key score matrix\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads * self.head_dim)\n",
    "\n",
    "        return self.fc_out(out)\n",
    "\n",
    "# DatePositionalEncoding模块\n",
    "def encode_cyclic_feature(value, max_value):\n",
    "    value = value.float()  # Ensure float for computation\n",
    "    sin_value = torch.sin(2 * np.pi * value / max_value)\n",
    "    cos_value = torch.cos(2 * np.pi * value / max_value)\n",
    "    return sin_value, cos_value\n",
    "\n",
    "class FeedForward_silu(nn.Module):\n",
    "    def __init__(self, embed_size, ff_hidden_size):\n",
    "        super(FeedForward_silu, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, ff_hidden_size)\n",
    "        self.fc2 = nn.Linear(ff_hidden_size, embed_size)\n",
    "        self.silu = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class DatePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(DatePositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        # Assuming you still want to encode year, month, and day\n",
    "        self.linear = nn.Linear(5, d_model)  # Input features: normalized_year, sin_month, cos_month, sin_day, cos_day\n",
    "\n",
    "    def forward(self, date_features):\n",
    "        \n",
    "        if isinstance(date_features, list):\n",
    "            date_features = torch.tensor(date_features, dtype=torch.float32)\n",
    "        # print(f\"{date_features.size()}\")\n",
    "        # date_features is expected to be a tensor with shape [batch_size, 3] where columns are year, month, day\n",
    "        batch_size = date_features.size(0)\n",
    "        year = date_features[:, :,0]\n",
    "        month = date_features[:,:, 1]\n",
    "        day = date_features[:, :,2]\n",
    "        \n",
    "        normalized_year = (year - 2012) / (2023 - 2012)  # Example normalization\n",
    "        \n",
    "        # Assuming encode_cyclic_feature is a function that encodes month and day as cyclic features\n",
    "        sin_month, cos_month = encode_cyclic_feature(month, 12)\n",
    "        sin_day, cos_day = encode_cyclic_feature(day, 31)\n",
    "\n",
    "        encoded_features = torch.stack([normalized_year, sin_month, \n",
    "                                        cos_month, sin_day, cos_day], dim=2)\n",
    "        \n",
    "        position_encodings = self.linear(encoded_features)  # Map to d_model dimensions\n",
    "        return position_encodings\n",
    "    \n",
    "class TransformerBlock_silu(nn.Module):\n",
    "    def __init__(self, embed_size, heads, ff_hidden_size, dropout_rate):\n",
    "        super(TransformerBlock_silu, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = FeedForward_silu(embed_size, ff_hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, value, key, query):\n",
    "        attention = self.attention(value, key, query)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "    \n",
    "class CrossingTransformer(nn.Module):\n",
    "    def __init__(self, embed_size, num_layers, heads, device, ff_hidden_size, dropout_rate, input_features):\n",
    "        super(CrossingTransformer, self).__init__()\n",
    "        self.device = device\n",
    "        self.positional_encoding = DatePositionalEncoding(embed_size)\n",
    "        self.layers = nn.ModuleList([TransformerBlock_silu(embed_size, heads, ff_hidden_size, dropout_rate) for _ in range(num_layers)])\n",
    "        self.feature_embedding = nn.Linear(input_features, embed_size)\n",
    "        self.output_layer = nn.Linear(embed_size, input_features)\n",
    "        self.gate = nn.Linear(embed_size*2, embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # data division\n",
    "        dates = x[:,:,:3]  # dates.shape:(batch_size, lookback, 3)\n",
    "        x = x[:,:, 3:]  # x.shape:(batch_size, lookback, features)\n",
    "        # print(f\"1 x.shape:{x.shape}\")\n",
    "        # feature embedding\n",
    "        x = self.feature_embedding(x) # x.shape:(lookback, batch_size, embed_size)\n",
    "        # positional encoding\n",
    "        positions = self.positional_encoding(dates).to(self.device) # positions.shape:(lookback, batch_size, embed_size)\n",
    "        x = x + positions\n",
    "        # transformer blocks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x)\n",
    "        # print(f\"2 x.shape:{x.shape}\")\n",
    "        ############################################################\n",
    "        #                  SPACIAL  TRANSFORMER                    #\n",
    "        ############################################################ \n",
    "        y = x.permute(1,0,2) # x.shape:(batch_size, lookback, features)\n",
    "        # print(f\"3 y.shape:{y.shape}\")\n",
    "                # transformer blocks\n",
    "        for layer in self.layers:\n",
    "            y = layer(y, y, y)\n",
    "\n",
    "        y = y.permute(1,0,2) # x.shape:(lookback, batch_size, features)    \n",
    "\n",
    "        # print(f\"4 y.shape:{y.shape}\")\n",
    "        ############################################################\n",
    "        #                  GATE  LAYER                             #\n",
    "        ############################################################\n",
    "\n",
    "        x_y = torch.cat((x,y),dim=-1)\n",
    "\n",
    "        # print(f\"x.shape:{x.shape}\")\n",
    "        # print(f\"y.shape:{y.shape}\")\n",
    "        # print(f\"x_y.shape:{x_y.shape}\")\n",
    "        \n",
    "        gate_weight = torch.sigmoid(self.gate(x_y))\n",
    "        output = gate_weight * x + (1-gate_weight) * y\n",
    "        \n",
    "        output = output[-1,:,:]\n",
    "        output = self.output_layer(output)\n",
    "        return output # return x.shape: (1, batch_size, features)\n",
    "    \n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.size(1)]\n",
    "       \n",
    "class VanillaTransformer(nn.Module):\n",
    "    def __init__(self, embed_size, num_layers, heads, device, ff_hidden_size, dropout_rate, input_features):\n",
    "        super(VanillaTransformer, self).__init__()\n",
    "        self.device = device\n",
    "        # self.positional_encoding = DatePositionalEncoding(embed_size)\n",
    "        self.positional_encodnig_original = PositionalEncoding(embed_size)\n",
    "        self.layers = nn.ModuleList([TransformerBlock_relu(embed_size, heads, ff_hidden_size, dropout_rate) for _ in range(num_layers)])\n",
    "        self.feature_embedding = nn.Linear(input_features, embed_size)\n",
    "        self.output_layer = nn.Linear(embed_size, input_features)\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # data division\n",
    "        # dates = x[:,:,:3]  # dates.shape:(lookback, batch_size, 3)\n",
    "        x = x[:,:, 3:]  # x.shape:(lookback, batch_size, features)\n",
    "        # feature embedding\n",
    "        x = self.feature_embedding(x)*np.sqrt(self.embed_size) # x.shape:(lookback, batch_size, embed_size)\n",
    "        # positional encoding\n",
    "        pos = self.positional_encodnig_original(x).to(self.device) \n",
    "        x = x + pos\n",
    "\n",
    "        # transformer blocks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x)\n",
    "        x = x[-1,:,:]\n",
    "        x = self.output_layer(x)\n",
    "        return x # return x.shape: (1, batch_size, features)\n",
    "\n",
    "    \n",
    "class FeedForward_relu(nn.Module):\n",
    "    def __init__(self, embed_size, ff_hidden_size):\n",
    "        super(FeedForward_relu, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, ff_hidden_size)\n",
    "        self.fc2 = nn.Linear(ff_hidden_size, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class TransformerBlock_relu(nn.Module):\n",
    "    def __init__(self, embed_size, heads, ff_hidden_size, dropout_rate):\n",
    "        super(TransformerBlock_relu, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = FeedForward_relu(embed_size, ff_hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, value, key, query):\n",
    "        attention = self.attention(value, key, query)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.counter = 0\n",
    "        self.EpochSleep = 700\n",
    "\n",
    "    def __call__(self, val_loss,epoch):\n",
    "        score = val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience and epoch > self.EpochSleep:\n",
    "                self.early_stop = True\n",
    "            return (f'EarlyStopping counter: {self.counter} out of {self.patience}', self.best_score)\n",
    "            \n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Show_TrainInfo(index_input, index_target, _input, target, stock_name,epoch,verbose=True):\n",
    "    print(f\"===================================TRAIN INFO / EPOCH: {epoch}===================================\")\n",
    "    print(f\"[Train Stock] ----- {stock_name[:]}\")\n",
    "    print(f\"[Input Shape] ----- {index_input.shape}\")\n",
    "    print(f\"[Stock Nums] ----- {index_input.shape[0]}\")\n",
    "    print(f\"[LookBack Len] ----- {index_input.shape[-1]}\")\n",
    "#     print(f\"[Start Date] ----- {_input[:,-1,:3].cpu().numpy().astype(int)}\")\n",
    "\n",
    "def Show_ValInfo(index_input, index_target, _input, target, stock_name,epoch,verbose=True):\n",
    "    print(f\"===================================VAL INFO / EPOCH: {epoch}===================================\")\n",
    "#     print(f\"[Train Stock] ----- {stock_name[:]}\")\n",
    "    print(f\"[Input Shape] ----- {index_input.shape}\")\n",
    "#     print(f\"[Stock Nums] ----- {index_input.shape[-1]}\")\n",
    "#     print(f\"[LookBack Len] ----- {index_input.shape[0]}\")\n",
    "#     print(f\"[Start Date] ----- {_input[:,-1,:3].cpu().numpy().astype(int)}\")\n",
    "    \n",
    "def record_data_directly(stock_names, date_tensors, train_info):\n",
    "    for idx, stock_name in enumerate(stock_names):\n",
    "        for date_tensor in date_tensors[idx]:\n",
    "            year, month, day = date_tensor.numpy().astype(int)\n",
    "            date_str = f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "            key = (stock_name, date_str)\n",
    "            \n",
    "            # 更新train_info中的计数\n",
    "            if key not in train_info:\n",
    "                train_info[key] = 1\n",
    "            else:\n",
    "                train_info[key] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel_RandomSampling(TrainLoader, TestLoader, Model, Optimizer, Criterion, Device, \n",
    "                              NumEpochs, EarlyStopPatience, LearningRate, Sample_K,MODELNAME, VerboseFreq=5):\n",
    "    \n",
    "    device = Device\n",
    "    model = Model.float().to(device)\n",
    "    criterion = Criterion\n",
    "    optimizer = Optimizer(model.parameters(), lr=LearningRate)\n",
    "\n",
    "    train_loader = TrainLoader\n",
    "    test_loader = TestLoader\n",
    "    early_stopping = EarlyStopping(patience=EarlyStopPatience, verbose=True)\n",
    "    num_epochs = NumEpochs\n",
    "    k = Sample_K\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_data_counts = {}\n",
    "    train_info = {}\n",
    "\n",
    "    for epoch in range(NumEpochs):\n",
    "        \n",
    "        print(f\"{epoch}/{NumEpochs}\")\n",
    "\n",
    "        ####################################### Training #######################################\n",
    "        model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for j, (index_input, index_target, _input, target, stock_name) in enumerate(train_loader):\n",
    "            # Show_TrainInfo(index_input, index_target, _input, target, stock_name, epoch)\n",
    "            record_data_directly(stock_name, _input[:,:, :3], train_info)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            src = _input.permute(1,0,2).float().to(device)[:-1,:,:]  # src.shape:(lookback, batch_size, features)\n",
    "\n",
    "            # print(f\"src.shape:{src.shape}\")\n",
    "            target = target.permute(1,0,2).float().to(device)[:,:,3:].view(-1,5) # target.shape:(predict_len, batch_size, features)\n",
    "            # print(f\"target.shape:{target.shape}\")\n",
    "            sampled_src = src[:1,:,:]\n",
    "            # print(f\"sampled_src.shape:{sampled_src.shape}\")\n",
    "            count_false = 0\n",
    "            \n",
    "            for i in range (src.shape[0]-1):\n",
    "                prediction = model(sampled_src) # prediction.shape: (batch_size, features)\n",
    "                if i<15:                            \n",
    "                        prob_true_val = True\n",
    "                else: \n",
    "                        v = k / (k + np.exp(epoch/k))  \n",
    "                        prob_true_val = np.random.choice([True, False], p=[v, 1-v])\n",
    "\n",
    "                if prob_true_val or count_false > 3: # use true value\n",
    "                        sampled_src = torch.cat((sampled_src.detach(), src[i+1,:,:].unsqueeze(0).detach()))\n",
    "                else: # use predicted value\n",
    "                        count_false += 1\n",
    "                        positional_encodings_new_val = src[i+1,:,:3].unsqueeze(0) #shape(1,13,3)                        \n",
    "                        predicted_features = torch.cat((positional_encodings_new_val, prediction.unsqueeze(0)), dim=2)\n",
    "                        sampled_src = torch.cat((sampled_src.detach(), predicted_features.detach()))\n",
    "\n",
    "            loss = criterion(target,prediction)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.detach().item()\n",
    "            # print(f\"len(target): {len(target)} / BATCH: {j} / EPOCH: {epoch} / ITERATION: {i}\")\n",
    "            # print(f\"\\r\\nEPOCH {epoch+1} \\nT_LOSS ----- [{round(total_loss/(j+1),3)}]\")\n",
    "        \n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        if epoch > 0:\n",
    "            train_losses.append(epoch_loss)\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            torch.save(model.state_dict(),f'{MODELNAME}_epoch{epoch}.pth')\n",
    "            \n",
    "\n",
    "\n",
    "        ####################################### Validation #######################################\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     for index_input, index_target, _input, target, stock_name in test_loader:\n",
    "        #         # Show_ValInfo(index_input, index_target, _input, target, stock_name,epoch)\n",
    "\n",
    "        #         src = _input.permute(1,0,2).float().to(device)[:-1,:,:]\n",
    "        #         target = target.permute(1,0,2).float().to(device)[:,:,3:].view(-1,5)\n",
    "                \n",
    "        #         prediction = model(src)\n",
    "        #         v_loss = criterion(target, prediction)\n",
    "        #         val_loss += v_loss.item()\n",
    "        # val_loss /= len(test_loader)\n",
    "        # if epoch > 0:\n",
    "        #     val_losses.append(val_loss)\n",
    "        \n",
    "        # print(f\"\\r\\nEPOCH {epoch+1} \\nT_LOSS ----- [{round(total_loss/(j+1),3)}] \\nV_LOSS ----- [{round(val_loss,3)}]\")\n",
    "\n",
    "\n",
    "        # ####################################### Early Stopping #######################################\n",
    "        # check = early_stopping(val_loss,epoch)\n",
    "        # print(check)\n",
    "        # if early_stopping.early_stop:\n",
    "        #     print(\"Early stopping\")\n",
    "        #     break\n",
    "\n",
    "        ####################################### Training Visualization #######################################\n",
    "        if (epoch + 1) % VerboseFreq == 0:\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=(20, 8))\n",
    "            # plt.xlim(0,700)\n",
    "            plt.plot(train_losses, label='Training Loss',color='blue',marker='o')\n",
    "            plt.plot(val_losses, label='Validation Loss',color='red', linestyle='dashed')\n",
    "            plt.legend()\n",
    "            plt.xlabel('Epoch = '+str(epoch+1))\n",
    "            plt.title('T Loss = '+str(round(epoch_loss,3)))\n",
    "            # plt.annotate(check, (0,0), (600, 400), xycoords='axes fraction', textcoords='offset points', va='top')\n",
    "            plt.show()\n",
    "            plt.clf() \n",
    "            \n",
    "    return model, train_losses, val_losses, train_info\n",
    "\n",
    "def Training_HeatMap(train_info):\n",
    "    train_info_list = [(stock_code, date, count) for (stock_code, date), count in train_info.items()]\n",
    "    train_info_dt = pd.DataFrame(train_info_list, columns=['Stock', 'Date', 'Count'])\n",
    "    train_info_dt_pivot = train_info_dt.pivot(index='Date', columns='Stock', values='Count').fillna(0)\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.heatmap(train_info_dt_pivot, annot=False, fmt=\".2f\", cmap=\"YlGnBu\")\n",
    "    plt.title('Stock Data Usage Heatmap')\n",
    "    plt.xlabel('Stock Code')\n",
    "    plt.ylabel('Date')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"DEVICE: {device}\")\n",
    "model_CrossingTransformer = CrossingTransformer(embed_size=256, num_layers=6, heads=8, device=device, ff_hidden_size=512, dropout_rate=0.1, input_features=5).float().to(device)\n",
    "model_VanillaTransformer = VanillaTransformer(embed_size=256, num_layers=12, heads=8, device=device, ff_hidden_size=512, dropout_rate=0.1, input_features=5).float().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "model_CrossingTransformer, train_losses_cstrans, val_losses_cstrans, train_info_cstrans = TrainModel_RandomSampling(TrainLoader=train_loader, TestLoader=test_loader, \n",
    "                                                                        Model=model_CrossingTransformer, Optimizer=optimizer, Criterion=criterion, Device=device, \n",
    "                                                                        NumEpochs=NUM_EPOCHS,EarlyStopPatience=EARLYSTOP_PATIENCE, \n",
    "                                                                        LearningRate=0.0001, Sample_K=K, MODELNAME='CrT',VerboseFreq=5)\n",
    "time_end = time.time()\n",
    "print('Time Cost:', time_end - time_start, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "model_VanillaTransformer, train_losses_vtrans, val_losses_vtrans, train_info_vtrans = TrainModel_RandomSampling(TrainLoader=train_loader, TestLoader=test_loader,\n",
    "                                                                        Model=model_VanillaTransformer, Optimizer=optimizer, Criterion=criterion, Device=device, \n",
    "                                                                        NumEpochs=NUM_EPOCHS, EarlyStopPatience=EARLYSTOP_PATIENCE, \n",
    "                                                                        LearningRate=0.0001, Sample_K=K, MODELNAME='VnlT',VerboseFreq=5)\n",
    "time_end = time.time()\n",
    "print('Time Cost:', time_end - time_start, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制训练热图、损失图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T05:39:35.336956200Z",
     "start_time": "2024-03-23T05:39:31.431031200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_info_list = [(stock_code, date, count) for (stock_code, date), count in train_info_cstrans.items()]\n",
    "train_info_dt = pd.DataFrame(train_info_list, columns=['Stock', 'Date', 'Count'])\n",
    "train_info_dt_pivot = train_info_dt.pivot(index='Date', columns='Stock', values='Count').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(figsize=(4, 5),dpi=3000) \n",
    "plt.style.use('seaborn-darkgrid')\n",
    "sns.heatmap(train_info_dt_pivot, annot=False, fmt=\".2f\", cmap=\"YlGnBu\")\n",
    "plt.title('Training Heatmap')\n",
    "plt.xlabel('Stock Code')\n",
    "# no x labels\n",
    "plt.xticks([])\n",
    "plt.ylabel('Date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T02:56:46.697795800Z",
     "start_time": "2024-03-22T02:56:46.678647100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(losses, label='Training Loss')\n",
    "# plt.title('Loss During Training')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.annotate(f\"K={K} / NumEpochs={NUM_EPOCHS} / SampleSize={SAMPLE_SIZE} / BatchSize={BATCH_SIZE} / LookBack={LOOK_BACK} / \\nStock: {stock_name[:]} \", (0,0), (0, -40), xycoords='axes fraction', textcoords='offset points', va='top')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TS-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model_CrossingTransformer.eval()\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for epoch in tqdm(range(50)):\n",
    "        for index_input, index_target, _input, target, stock_name in test_loader:\n",
    "\n",
    "            src = _input.permute(1,0,2).float().to(device)[:-1,:,:]\n",
    "            target = target.permute(1,0,2).float().to(device)[:,:,3:].view(-1,5)\n",
    "            # print(f\"***target shape:{torch.exp(target)}***\")     \n",
    "    \n",
    "            prediction = model_CrossingTransformer(src)\n",
    "            # print(f\"***prediction shape:{torch.exp(prediction)}***\")\n",
    "\n",
    "            predictions.append(prediction.cpu().numpy())\n",
    "            actuals.append(target.cpu().numpy())\n",
    "\n",
    "            # print((torch.exp(target)-torch.exp(prediction))/torch.exp(target))\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "actuals = np.array(actuals)\n",
    "\n",
    "mse = mean_squared_error(actuals.reshape(-1,5), predictions.reshape(-1,5))\n",
    "mae = mean_absolute_error(actuals.reshape(-1,5), predictions.reshape(-1,5))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VanTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_VanillaTransformer.eval()\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for epoch in tqdm(range(50)):\n",
    "        for index_input, index_target, _input, target, stock_name in test_loader:\n",
    "\n",
    "            src = _input.permute(1,0,2).float().to(device)[:-1,:,:]\n",
    "            target = target.permute(1,0,2).float().to(device)[:,:,3:].view(-1,5)\n",
    "            # print(f\"***target shape:{torch.exp(target)}***\")     \n",
    "\n",
    "            prediction = model_VanillaTransformer(src)\n",
    "            # print(f\"***prediction shape:{torch.exp(prediction)}***\")\n",
    "\n",
    "            predictions.append(prediction.cpu().numpy())\n",
    "            actuals.append(target.cpu().numpy())\n",
    "\n",
    "            # print((torch.exp(target)-torch.exp(prediction))/torch.exp(target))\n",
    "predictions = np.array(predictions)\n",
    "actuals = np.array(actuals)\n",
    "mse = mean_squared_error(actuals.reshape(-1,5), predictions.reshape(-1,5))\n",
    "mae = mean_absolute_error(actuals.reshape(-1,5), predictions.reshape(-1,5))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_CrossingTransformer.eval()\n",
    "# # test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "# predictions = []\n",
    "# actuals = []\n",
    "\n",
    "# # with torch.no_grad():\n",
    "\n",
    "#     for epoch in range(3):\n",
    "#         for index_input, index_target, _input, target, stock_name in test_loader:\n",
    "\n",
    "#             src = _input.permute(1,0,2).float().to(device)[:-1,:,:]\n",
    "#             target = target.permute(1,0,2).float().to(device)[:,:,3:].view(-1,5)\n",
    "#             print(f\"***target shape:{torch.exp(target)}***\")     \n",
    "\n",
    "#             prediction = model_CrossingTransformer(src)\n",
    "#             print(f\"***prediction shape:{torch.exp(prediction)}***\")\n",
    "\n",
    "#             predictions.append(prediction.cpu().numpy())\n",
    "#             actuals.append(target.cpu().numpy())\n",
    "\n",
    "#             print(torch.exp(target)-torch.exp(prediction))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index_input, index_target, _input, target, stock_name in train_loader:\n",
    "#     # Show_ValInfo(index_input, index_target, _input, target, stock_name,epoch)\n",
    "#     # 对验证数据进行相同的数据处理\n",
    "#     src = _input.permute(1,0,2).float().to(device)[:-1,:,:]\n",
    "#     target = target.permute(1,0,2).float().to(device)[:,:,3:].view(-1,6)\n",
    "#     # 进行预测\n",
    "#     prediction = model(src)\n",
    "#     print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(predictions[:100,0,0], label='Actuals')\n",
    "# # plt.plot(predictions[0][:,:], label='Predictions')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Value')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "plt.figure(figsize=(5, 2.65), dpi=2000)\n",
    "plt.style.use(['ieee','science'])\n",
    "plt.plot(train_losses_cstrans, label='TS-Transformer')\n",
    "plt.plot(train_losses_vtrans, label='Vanilla Transformer')\n",
    "plt.title('Training Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(0, 3)\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
